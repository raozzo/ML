{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamentals of Information Systems\n",
    "\n",
    "## Python Programming (for Data Science)\n",
    "\n",
    "### Master's Degree in Data Science\n",
    "\n",
    "#### Giorgio Maria Di Nunzio\n",
    "#### (Courtesy of Gabriele Tolomei FIS 2018-2019)\n",
    "<a href=\"mailto:giorgiomaria.dinunzio@unipd.it\">giorgiomaria.dinunzio@unipd.it</a><br/>\n",
    "University of Padua, Italy<br/>\n",
    "2021/2022<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 9: Data Preparation with <code>pandas</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Does Data Preparation Mean?\n",
    "\n",
    "-  It typically consists of **loading**, **cleaning**, **transforming**, and **rearranging** data (the last three steps are also referred to as data **\"munging\"**). \n",
    "\n",
    "-  Such tasks are often reported to take up 80% or more of your development time (for a machine learning/data science task). \n",
    "\n",
    "-  Sometimes it can be achieved by using a mixture of tools, i.e., from general-purpose programming languages, like Python, Perl, R, or Java, to UNIX tools like <code>**sed**</code> or <code>**awk**</code>.\n",
    "\n",
    "-  Luckily, <code>**pandas**</code> provides you with a single, high-level, flexible, and fast set of tools to enable you to manipulate data into the right form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our Focus\n",
    "\n",
    "1.  Handling missing data (**NA** or **N**ot **A**vailable)\n",
    "\n",
    "2.  Dealing with duplicates\n",
    "\n",
    "3.  Managing very extreme values (i.e., **outliers**)\n",
    "\n",
    "4.  Combining multiple datasets into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before we start our journey on data preparation with pandas,\n",
    "we get back to the example we used in Lecture 08.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user'\n",
    "# The first line of the file represents the header, and each field\n",
    "# is separated by a pipe\n",
    "\"\"\"\n",
    "We specify the url where the data is located, the character u|sed to separate fields ('|')\n",
    "and the name of the column to use as row label (otherwise, RangeInteger will be used)\n",
    "\"\"\"\n",
    "users = pd.read_csv(url, sep='|', index_col='user_id')\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Missing Data (*NA*)\n",
    "\n",
    "-  Missing data (**NA**) may either be data that does not exist or that exists but was not observed (e.g., due to measurement issues).\n",
    "\n",
    "-  <code>**pandas**</code> makes working with missing data as painless as possible. For example, all of the descriptive statistics on <code>**pandas**</code> objects exclude **NA** by default.\n",
    "\n",
    "-  <code>**pandas**</code> represents **NA** using so-called **sentinel values**.\n",
    "\n",
    "-  Two of the most common sentinel values are <code>**None**</code> and the floating point value <code>**NaN**</code> (**N**ot **a** **N**umber)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see if there are any missing data (NA) on our loaded dataset.\n",
    "\"\"\"\n",
    "\n",
    "# isnull() returns a boolean DataFrame with the same shape of the DataFrame object\n",
    "# where you invoke the method. Each entry of this new boolean DataFrame either contains\n",
    "# True or False depending on whether the corresponding entry in the original DataFrame\n",
    "# is missing or not\n",
    "print(\"Shape of the boolean DataFrame: {}\\n\".format(users.isnull().shape))\n",
    "print(users.isnull().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to see which column has at least one missing value,\n",
    "we can call the method any() on the boolean DataFrame above.\n",
    "This returns a boolean Series which contains an entry for each column \n",
    "(row aggregation) which evaluates to True if at least one element\n",
    "of that column is True, False otherwise.\n",
    "\"\"\"\n",
    "print(\"Shape of the boolean Series: {}\\n\".format(users.isnull().any().shape))\n",
    "print(users.isnull().any().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have verified that our dataset does not contain any missing value.\n",
    "What if, though, I would like to obtain a row-wise (i.e., column aggregation)\n",
    "boolean Series containing a value for each row, which tells me whether that\n",
    "row contains or not at least one NA value?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Shape of the boolean Series: {}\\n\".format(users.isnull().any(axis=1).shape))\n",
    "print(users.isnull().any(axis=1).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to find whether there exists at least one NA on the whole DataFrame\n",
    "we can simply aggregate one more time the boolean Series above using another any()\n",
    "\"\"\"\n",
    "\n",
    "print(\"Q: Is there at least a missing value in our DataFrame? A: {}\".\n",
    "      format(users.isnull().any().any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Of course, the same thing can be achieved using different approaches.\n",
    "This is possibly the quickest solution.\n",
    "\"\"\"\n",
    "\n",
    "# values returns a 2-D numpy array (ndarray) and any() \"flatten\" it \n",
    "print(\"Q: Is there at least a missing value in our DataFrame? A: {}\".\n",
    "      format(users.isnull().values.any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This solution makes exactly the same 3 steps as above but instead of computing\n",
    "boolean aggregation from the boolean DataFrame, it sum boolean values over the rows\n",
    "(i.e., column-wise) and finally makes a final aggregation step (sum) to obtain\n",
    "the number of missing values of the DataFrame.\n",
    "NOTE: this is generally slower but it provides you with an extra information\n",
    "(i.e., how many NA values are in the DataFrame, not just a boolean value!)\n",
    "\"\"\"\n",
    "print(\"There are {} missing values (NA) in the DataFrame\".\n",
    "      format(users.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Suppose we want to randomly perturbate our dataset with some missing values.\n",
    "First of all, let's create a deep copy of our original DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the deep copy of the DataFrame where we are going to randomly insert\n",
    "# some missing values (NA)\n",
    "users_with_na = users.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's create a uniform random sample of size=10 drawn from the range [1, 943]\n",
    "\"\"\"\n",
    "np.random.seed(42)\n",
    "row_indices = np.random.randint(low = 1, high = 944, size = 10)\n",
    "# alternatively, use np.random.choice(np.arange(1, 944), size=10, replacement=False)\n",
    "print(\"Random row indices: {}\".format(row_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We are going to use the first half of the randomly selected indices\n",
    "for populating with np.NaN the column 'age', whilst the second half\n",
    "is going to be used to set to None the column 'occupation'.\n",
    "The two middle indices will be used for both. \n",
    "In other words:\n",
    "- rows labeled as 103, 436, 861, 271 will be used to\n",
    "set column 'age' to np.NaN\n",
    "- rows labeled as 701, 21, 615, 122 will be used to\n",
    "set column 'occupation' as None\n",
    "- rows labeled as 107 and 72 will be used to set both\n",
    "column 'age' to np.NaN and column 'occupation' to None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's first extract the records we want to update for column 'age':\\n{}\".\n",
    "      format(users_with_na.loc[row_indices[:6], 'age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's first extract the records we want to update for column 'age':\\n{}\".\n",
    "      format(users_with_na.loc[row_indices[:6], 'age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Let's now extract the records we want to update for column 'occupation':\\n{}\".\n",
    "      format(users_with_na.loc[row_indices[4:], 'occupation']))\n",
    "# I swear, I didn't do it on purpose! \n",
    "# This was truly the outcome of a purely (pseudo-)random experiment but apparently\n",
    "# 'scientist' will be sacrificed and set to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's set the values as we planned.\n",
    "\"\"\"\n",
    "users_with_na.loc[row_indices[:6], 'age'] = np.nan\n",
    "users_with_na.loc[row_indices[4:], 'occupation'] = None\n",
    "# NOTE: the same won't work if we use something like the following:\n",
    "# users_with_na.loc[row_indices[:6]]['age'] = np.nan\n",
    "# users_with_na.loc[row_indices[4:]]['occupation'] = None\n",
    "# This is because in case of [] operator, we are actually \n",
    "# accessing a copy of the selected slice, whereas above we are working on a view.\n",
    "# More information on this - a.k.a. SettingWithCopyWarning - is available here: \n",
    "# https://www.dataquest.io/blog/settingwithcopywarning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's verify if changes actually took place!\n",
    "\"\"\"\n",
    "print(\"Let's see how column 'age' looks like:\\n{}\".\n",
    "      format(users_with_na.loc[row_indices[:7], 'age']))\n",
    "print()\n",
    "print(\"Let's see how column 'occupation' looks like:\\n{}\".\n",
    "      format(users_with_na.loc[row_indices[3:], 'occupation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, let's try to see if the tests above we run for finding any NA work correctly.\n",
    "\"\"\"\n",
    "print(\"Q: Is there at least a missing value in our DataFrame? A: {}\".\n",
    "      format(users_with_na.isnull().any().any()))\n",
    "print()\n",
    "print(\"There are {} missing values (NA) in the DataFrame\".\n",
    "      format(users_with_na.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filtering Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You may want to drop rows or columns of a DataFrame which are all NA \n",
    "or only those containing any NAs. \n",
    "The pandas.dropna function by default drops any row containing a missing value.\n",
    "By default, dropna returns a new object but we can specify inplace=True for\n",
    "any in-place change.\n",
    "\"\"\"\n",
    "\n",
    "# This will drop all the rows containing at least one NA\n",
    "cleaned_users = users_with_na.dropna()\n",
    "\n",
    "# After issuing the above command, we are expecting the DataFrame to have 10 rows less\n",
    "# (i.e., 933 instead of 943)\n",
    "print(\"Number of records in the original DataFrame = {}\\n\".format(users_with_na.shape[0]))\n",
    "print(\"Number of records in the cleaned DataFrame = {}\".format(cleaned_users.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we instead want to delete only those rows which are FULL of NAs,\n",
    "then we have to pass how='all' to dropna.\n",
    "\"\"\"\n",
    "# This will drop all the rows containing ALL NAs\n",
    "cleaned_users_all = users_with_na.dropna(how = 'all')\n",
    "\n",
    "# After issuing the above command, how many rows will be dropped?\n",
    "print(\"Number of records in the original DataFrame = {}\\n\".format(users_with_na.shape[0]))\n",
    "print(\"Number of records in the cleaned DataFrame = {}\".format(cleaned_users_all.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As usual, dropna works on rows (axis 0) by default.\n",
    "Instead, if we want to delete columns corresponding to a missing value\n",
    "we can pass axis=1 argument to dropna.\n",
    "\"\"\"\n",
    "# This will drop all the columns containing at least one NA\n",
    "cleaned_users_columns = users_with_na.dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# How many columns are we expecting the above command will drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# After issuing the above command, weare expecting the DataFrame to have 2 columns less\n",
    "# (i.e., 2 instead of 4, as 'age' and 'occupation' have both at least one NA value)\n",
    "print(\"Number of columns in the original DataFrame = {}\\n\".format(users_with_na.shape[1]))\n",
    "print(\"Number of columns in the cleaned DataFrame = {}\".format(cleaned_users_columns.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filling in Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'fillna' fill in missing data with some values, rather than filtering them out.\n",
    "\"\"\"\n",
    "# Suppose we fill all missing values with 0 (not in-place, otherwise set inplace=True)\n",
    "users_fill_na = users_with_na.fillna(0)\n",
    "\n",
    "# Let's verify if changes actually took place!\n",
    "print(\"Let's see how column 'age' looks like:\\n{}\".\n",
    "      format(users_fill_na.loc[row_indices[:6], 'age']))\n",
    "print()\n",
    "print(\"Let's see how column 'occupation' looks like:\\n{}\".\n",
    "      format(users_fill_na.loc[row_indices[4:], 'occupation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can also pass a dictionary to fillna in order to specify\n",
    "different values to replace NA with.\n",
    "\"\"\"\n",
    "users_fill_na = users_with_na.fillna({'age': 0, 'occupation': 'none'})\n",
    "# Let's verify if changes actually took place!\n",
    "print(\"Let's see how column 'age' looks like:\\n{}\".\n",
    "      format(users_fill_na.loc[row_indices[:6], 'age']))\n",
    "print()\n",
    "print(\"Let's see how column 'occupation' looks like:\\n{}\".\n",
    "      format(users_fill_na.loc[row_indices[4:], 'occupation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The DataFrame method duplicated() returns a boolean Series \n",
    "indicating whether each row is a duplicate or not.\n",
    "\"\"\"\n",
    "# Let's go back to our original DataFrame\n",
    "print(\"Duplicated rows:\\n{}\".format(users.duplicated().head()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# What if I would like to see if there exists at least one duplicated row?\n",
    "print(\"Q: Is there at least one duplicated row? A: {}\"\n",
    "      .format(users.duplicated().any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose I want to extract only the duplicated rows\n",
    "print(\"Duplicated rows:\\n{}\".format(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose I want to extract only the duplicated rows\n",
    "print(\"Duplicated rows:\\n{}\".format(users[users.duplicated()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By default, if you have 3 duplicated rows, only the last two will be\n",
    "marked as duplicates (i.e., the first occurrence is kept).\n",
    "This can be changed by specifying the parameters 'keep'\n",
    "keep : {'first', 'last', False}, default 'first'\n",
    "first : Mark duplicates as True except for the first occurrence.\n",
    "last : Mark duplicates as True except for the last occurrence.\n",
    "False : Mark all duplicates as True.\n",
    "\"\"\"\n",
    "# Suppose I want to extract only the duplicated rows, this time considering them all\n",
    "print(\"Duplicated rows:\\n{}\".format(users[users.duplicated(keep = False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Relatedly, drop_duplicates() returns a DataFrame where the duplicated array is False.\n",
    "\"\"\"\n",
    "# Remove duplicated rows (keeping the first occurrence of each duplicates)\n",
    "users_with_no_dup = users.drop_duplicates()\n",
    "# What if I would like to see if there exists at least one duplicate row, now?\n",
    "print(\"Q: Is there at least one duplicated row? A: {}\"\n",
    "      .format(users_with_no_dup.duplicated().any()))\n",
    "print()\n",
    "print(\"Total number of rows after removing duplicated rows = {}\"\n",
    "      .format(users_with_no_dup.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can also specify the same 'keep' argument to decide on how to\n",
    "mark duplicates, and therefore remove them\n",
    "\"\"\"\n",
    "# Remove ALL duplicated rows\n",
    "users_with_no_dup = users.drop_duplicates(keep = False)\n",
    "# What if I would like to see if there exists at least one duplicate row, now?\n",
    "print(\"Q: Is there at least one duplicated row? A: {}\"\n",
    "      .format(users_with_no_dup.duplicated().any()))\n",
    "print()\n",
    "print(\"Total number of rows after removing duplicated rows = {}\"\n",
    "      .format(users_with_no_dup.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By default, both duplicated() and drop_duplicates() consider all of the columns; \n",
    "alternatively we can specify any subset of them to detect duplicates. \n",
    "Suppose we want to filter duplicates only based on the 'gender' and 'occupation' columns.\n",
    "\"\"\"\n",
    "# Suppose I want to extract only the duplicated rows w.r.t. 'gender' and 'occupation'\n",
    "print(\"There are {} duplicated rows having the same 'gender' and 'occupation'.\\n\\\n",
    "The following are the first 5 of them:\\n{}\"\n",
    "      .format(users[users.duplicated(['gender', 'occupation'])].shape[0],\n",
    "              users[users.duplicated(['gender', 'occupation'])].head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transforming Data Using a Function or Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For many data sets, we may wish to perform some transformation based on the values\n",
    "in an array, Series, or column in a DataFrame.\n",
    "\"\"\"\n",
    "# Suppose we want to add a column indicating the salary for each occupation. \n",
    "# Let's write down a mapping of each occupation to salary\n",
    "occupation_to_salary = {'technician': 25000, 'administrator': 150000,\n",
    "                        'writer': 40000, 'executive': 300000, 'other': 18000,\n",
    "                        'student': 1300, 'lawyer': 27500, 'educator': 45000,\n",
    "                        'scientist': 60000, 'entertainment': 185000, 'programmer': 55000,\n",
    "                        'librarian': 22000, 'homemaker': 240000, 'artist': 72000,\n",
    "                        'engineer': 91000, 'marketing': 66000, 'none': 0,\n",
    "                        'healthcare': 41000, 'retired': 52000, 'salesman': 48000,\n",
    "                        'doctor': 140000\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_to_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users['occupation'].map(occupation_to_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The map method on a Series accepts a function or dict-like object containing a mapping.\n",
    "\"\"\"\n",
    "users['salary'] = users['occupation'].map(occupation_to_salary)\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Alternatively, we could also have passed to map a lambda function that does all the work.\n",
    "\"\"\"\n",
    "# Let's first delete the salary column\n",
    "del users['salary']\n",
    "# Verify the column is really removed\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# lambda function\n",
    "users['salary'] = users['occupation'].map(lambda x: occupation_to_salary[x])\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# lambda function\n",
    "users['salary'] = users['occupation'].map(lambda o: occupation_to_salary[o])\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Replacing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filling in missing data with the 'fillna' method is a special case \n",
    "of more general value replacement. \n",
    "While 'map' can be used to modify a subset of values in an object, \n",
    "'replace' provides a simpler and more flexible way to do so.\n",
    "\"\"\"\n",
    "# Suppose we would like to consider 'none' value of 'occupation' as missing data\n",
    "# (i.e., 'none' can be considered as a sentinel value)\n",
    "# NOTE: remember that this can be also specified when loading the DataFrame\n",
    "# by specifying na_values\n",
    "users['occupation'] = users['occupation'].replace('none', np.nan)\n",
    "print(\"The number of replaced rows is = {}\"\n",
    "      .format(users[(users['occupation'].isnull())].shape[0]))\n",
    "print(\"The following are the first 5 rows that have been replaced:\\n{}\"\n",
    "      .format(users[(users['occupation'].isnull())].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify that 'occupation' contains actually some NaN\n",
    "print(\"Q: Is there any missing value for 'occupation'? A: {}\".\n",
    "      format(users['occupation'].isnull().any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose I want to update the salary column corresponding to\n",
    "# those rows where 'occupation' is now missing\n",
    "mask = users['occupation'].notnull()\n",
    "# Update 'salary' where 'occupation' is null. The semantics is as follows.\n",
    "# 'salary' will keep its value if the mask condition is verified\n",
    "# (i.e., if 'occupation' is NOT null, otherwise we set it to NaN)\n",
    "users['salary'] = users['salary'].where(mask, np.nan)\n",
    "#np.where(mask, users.salary, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The number of replaced rows is = {}\"\n",
    "      .format(users[(users['occupation'].isnull()) & (users['salary'].isnull())].shape[0]))\n",
    "print(\"The following are the first 5 rows that have been replaced:\\n{}\"\n",
    "      .format(users[(users['occupation'].isnull()) & (users['salary'].isnull())].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's go back to the original occupation and salary\n",
    "users = users.fillna({'occupation': 'none'})\n",
    "users['salary'] = users['occupation'].map(lambda o: occupation_to_salary[o])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discretization and Binning\n",
    "\n",
    "-  Continuous data is often **discretized** or otherwised separated into \"**bins**\" for analysis. \n",
    "\n",
    "-  The typical example is given by the case where you have user's data containing information like 'age', and you what to divide users in a set of fixed age intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's consider our 'age' column and suppose we want to divide these into bins, \n",
    "such as users aged between 0 and 17, 18 to 25, 26 to 35, \n",
    "36 to 55, and finally 56 and older. To do so, we can use pandas.cut function.\n",
    "\"\"\"\n",
    "# Let's first define a list containing the left-most extreme of each bin\n",
    "bins = [0, 18, 26, 36, 56, 100]\n",
    "\"\"\"\n",
    "Consistent with mathematical notation for intervals, a parenthesis ')' \n",
    "means that the side is OPEN while the square bracket '[' means it is closed (inclusive). \n",
    "By default, intervals are left-open, i.e., (a, b]\n",
    "Passing 'right=False' those become right-open, i.e., [a, b) \n",
    "\"\"\"\n",
    "age_intervals = pd.cut(users['age'], bins, right=False)\n",
    "print(\"Ages: {}\".format(users['age'].head()))\n",
    "print(\"Categorical ranges: {}\".format(age_intervals.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The object pandas returns is a special Categorical object. \n",
    "We can treat it like an array of strings indicating the bin name; \n",
    "internally it contains a categories array indicating the distinct category names \n",
    "along with a labeling for the ages data in the 'codes' attribute\n",
    "\"\"\"\n",
    "print(\"Categorical bin codes: {}\".format(age_intervals.cat.codes.head()))\n",
    "print(\"Categorical bin names: {}\".format(age_intervals.cat.categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Instead of integer labeling for the bin, we can specify which label\n",
    "to assign to each bin.\n",
    "\"\"\"\n",
    "age_labels = ['Young', 'Young_Adult', 'Adult', 'Middle_Aged', 'Senior']\n",
    "\n",
    "age_intervals = pd.cut(users['age'], bins, labels=age_labels, right=False)\n",
    "\n",
    "print(\"Ages: {}\".format(users['age'].head()))\n",
    "print(\"Categorical ranges: {}\".format(age_intervals.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's create an extra column 'age_interval' on the DataFrame with this information.\n",
    "\"\"\"\n",
    "users['age_interval'] = pd.cut(users['age'], bins, labels=age_labels, right=False)\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sometimes we don't want to specify the intervals ourselves, instead\n",
    "we want to just specify the number of bins and let pandas figure out how \n",
    "data are distributed across those bins.\n",
    "\"\"\"\n",
    "# If we pass 'cut' an integer number of bins instead of explicit bin edges, \n",
    "# it will compute equal-length bins based on the minimum and maximum values in the data. \n",
    "age_intervals = pd.cut(users['age'], 5, right=False)\n",
    "print(\"Ages: {}\".format(users['age'].head()))\n",
    "print(\"Categorical ranges: {}\".format(age_intervals.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The same as above, but with labels\n",
    "age_labels = ['Young', 'Young_Adult', 'Adult', 'Middle_Aged', 'Senior']\n",
    "age_intervals = pd.cut(users['age'], 5, labels=age_labels, right=False)\n",
    "print(\"Ages: {}\".format(users['age'].head()))\n",
    "print(\"Categorical ranges: {}\".format(age_intervals.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A closely related function 'qcut' bins the data based on sample quantiles. \n",
    "Depending on the distribution of the data, using 'cut' will not usually result \n",
    "in each bin having the same number of data points. \n",
    "Instead, with 'qcut' by definition we will obtain roughly equal-size bins.\n",
    "\"\"\"\n",
    "# We are using quartiles here [0.25, 0.50, 0.75, 1]\n",
    "# Alternatively, we can specify the list of our own quantiles \n",
    "# (i.e., numbers between 0 and 1, inclusive)\n",
    "age_intervals = pd.qcut(users['age'], 4)\n",
    "print(\"Ages: {}\".format(users['age'].head()))\n",
    "print(\"Categorical ranges: {}\".format(age_intervals.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The same as above yet with labels\n",
    "age_labels = ['Young', 'Adult', 'Middle_Aged', 'Senior']\n",
    "age_intervals = pd.qcut(users['age'], 4, labels=age_labels)\n",
    "print(\"Ages: {}\".format(users['age'].head()))\n",
    "print(\"Categorical ranges: {}\".format(age_intervals.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detecting and Filtering Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's see the output of the 'describe()' function on our DataFrame.\n",
    "\"\"\"\n",
    "print(users.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Suppose we want to see what are the records where 'salary' is greater than 280k.\n",
    "\"\"\"\n",
    "salary_outlier = users.salary > 280000\n",
    "print(\"There are {} salary outliers, 5 of which are as follows:\\n{}\"\n",
    "      .format(users[salary_outlier].shape[0], users[salary_outlier].head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we want to change the value of an outlier with a 'cap' value (e.g., 280000)\n",
    "\"\"\"\n",
    "# 1. Using 'where'\n",
    "users.salary = users.salary.where(~salary_outlier, 280000)\n",
    "print(users[salary_outlier].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's revert back to the original salary\n",
    "\"\"\"\n",
    "users.salary = users.salary.where(~salary_outlier, 300000)\n",
    "print(users[salary_outlier].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we want to change the value of an outlier with a 'cap' value (e.g., 280000)\n",
    "\"\"\"\n",
    "# 2. Using 'loc'\n",
    "users.loc[salary_outlier, 'salary'] = 280000\n",
    "print(users[salary_outlier].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining <code>pandas</code> Objects\n",
    "\n",
    "-  Data contained in <code>**pandas**</code> objects (i.e., <code>**Series**</code> and <code>**DataFrame**</code>) can be combined together in a number of built-in ways, such as:\n",
    "\n",
    "    -  <code>**pandas.merge**</code>/<code>**pandas.join**</code> connects rows of two objects based on one or more keys. This is equivalent to **join** operations on relational databases.\n",
    "\n",
    "    -  <code>**pandas.concat**</code> concatenates or \"stacks\" together objects along a specific axis, if any (there is only a single possible axis of concatenation for <code>**Series**</code>).\n",
    "\n",
    "-  Full API documentation is available [here](https://pandas.pydata.org/pandas-docs/stable/merging.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Database-style <code>DataFrame</code> Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge or join operations combine data sets by linking rows using one or more keys. \n",
    "These operations are central to relational databases (e.g., SQL-based). \n",
    "The 'merge' function in pandas is the main entry point for achieving this.\n",
    "\"\"\"\n",
    "# Let's go back to our original DataFrame\n",
    "del users['salary']\n",
    "del users['age_interval']\n",
    "print(users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Join/Merge Operation?\n",
    "\n",
    "-  Generally speaking, \"joining\" (\"merging\") two datasets is the process of bringing two datasets together into one, and aligning the rows from each based on common attributes or columns.\n",
    "\n",
    "-  Typical operation in relational databases using <code>**SQL JOIN**</code> operator.\n",
    "\n",
    "-  In <code>**pandas**</code> there are two different functions <code>**merge**</code> and <code>**join**</code>, both of which do similar things; the former is used for column-to-column joins, whereas the latter is more efficient when joining <code>**DataFrame**</code> objects on their indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Anatomy of a Join/Merge Operation\n",
    "\n",
    "```python\n",
    "merged_df = pd.merge(left_df, right_df, \n",
    "                     left_on=[\"l_col_1\",...,\"l_col_n\"], \n",
    "                     right_on=[\"r_col_1\",...,\"r_col_n\"],\n",
    "                     how=\"{left|right|inner|outer}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Anatomy of a Join/Merge Operation\n",
    "\n",
    "-  <code>**left_df**</code> and <code>**right_df**</code> are the two <code>**DataFrame**</code> objects we want to merge.\n",
    "\n",
    "-  <code>**left_on**</code> and <code>**right_on**</code> indicate the column(s) used for the merging operation on the left and right <code>**DataFrame**</code> objects, respectively (alternatively, use just <code>**on=[col_1,...,col_n]**</code> if column names are the same on both <code>**DataFrame**</code>s).\n",
    "\n",
    "-  <code>**how**</code> is used to specify which kind of merge operation needs to be performed, i.e., one of: <code>**left**</code>, <code>**right**</code>, <code>**inner**</code> (default), and <code>**outer**</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different Types of Join/Merge Operations\n",
    "\n",
    "(./img/join_types.jpg)\n",
    "\n",
    "<center>[Image Source](https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "-  **Inner Merge/Inner Join** (default): Keep only those rows where the value which we want to merge on exists in **both** the **left** and **right** <code>**DataFrame**</code>s.\n",
    "\n",
    "-  **Left Merge/Left (Outer) Join**: Keep every row in the **left** <code>**DataFrame**</code>. If some values of the column we are merging on are missing in the **right** <code>**DataFrame**</code>, add <code>**NaN**</code> to the result.\n",
    "\n",
    "-  **Right Merge/Right (Outer) Join**: Keep every row in the **right** <code>**DataFrame**</code>. If some values of the column we are merging on are missing in the **left** <code>**DataFrame**</code>, add <code>**NaN**</code> to the result.\n",
    "\n",
    "-  **Outer Merge/Full (Outer) Join**: Return **all** the rows from the **left** and the **right** <code>**DataFrame**</code>s, matches up rows where possible, otherwise add <code>**NaN**</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose we have another DataFrame containing the (average) salary for some occupations. \n",
    "# Let's assume we don't have salary information for 'technician' and 'other'\n",
    "# but we do for two extra occupations (e.g., 'gardener' and 'professor')\n",
    "occupation_salary_data = {'occupation': ['administrator', 'writer', 'executive', \n",
    "                                       'student', 'lawyer', 'educator', \n",
    "                                       'scientist', 'entertainment', \n",
    "                                       'programmer', 'librarian', 'homemaker', \n",
    "                                       'artist', 'engineer', 'marketing', \n",
    "                                       'none', 'healthcare', 'retired', \n",
    "                                       'salesman', 'doctor', 'gardener', \n",
    "                                       'professor'],\n",
    "        'salary': [150000, 40000, 300000, 1300, 27500, 45000, \n",
    "                      60000, 185000, 55000, 22000, 240000, 72000, \n",
    "                      91000, 66000, 0, 41000, 52000, 48000, \n",
    "                      140000, 16000, 82000]}\n",
    "\n",
    "occupation_salary = pd.DataFrame(occupation_salary_data)\n",
    "print(\"Occupation-Salary data:\\n{}\".format(occupation_salary.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By default, 'merge' tries to join DataFrames on the basis of common column names.\n",
    "In our example there is a column called 'occupation', which is common to both DataFrames.\n",
    "Moreover, 'merge' implements an INNER JOIN, which means that the resulting DataFrame \n",
    "will contain all and only those records which match on both the left and right DataFrame. \n",
    "\"\"\"\n",
    "merged = pd.merge(users, occupation_salary)\n",
    "# The above is equivalent to:\n",
    "# merged = users.merge(occupation_salary)\n",
    "# Or:\n",
    "# merged = pd.merge(users, occupation_salary, \n",
    "#                   left_on=\"occupation\", right_on=\"occupation\", how=\"inner\")\n",
    "# Or, again:\n",
    "# merged = pd.merge(users, occupation_salary, on=\"occupation\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of records in the left DataFrame: {}\"\n",
    "      .format(users.shape[0]))\n",
    "print(\"Number of records in the right DataFrame: {}\"\n",
    "      .format(occupation_salary.shape[0]))\n",
    "print(\"Unique values of 'occupation' in the left DataFrame:\\n{}\".\n",
    "      format(users.occupation.unique()))\n",
    "print(\"Unique values of 'occupation' in the right DataFrame:\\n{}\".\n",
    "      format(occupation_salary.occupation.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of records in the resulting merged DataFrame: {}\"\n",
    "      .format(merged.shape[0]))\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Double check if the number of resulting rows after (inner) merge is compliant with\n",
    "what we expect. Remember, inner merge results in a number of records which correspond\n",
    "to the intersection of values on which we merge the 2 DataFrames on.\n",
    "\"\"\"\n",
    "# Let's see how many occupation values in the left DataFrame (users) \n",
    "# appear also in the right DataFrame (occupation_salary)\n",
    "print(\"Number of 'occupation' values shared between the two DataFrames:\\n{}\"\n",
    "      .format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Double check if the number of resulting rows after (inner) merge is compliant with\n",
    "what we expect. Remember, inner merge results in a number of records which correspond\n",
    "to the intersection of values on which we merge the 2 DataFrames on.\n",
    "\"\"\"\n",
    "# Let's see how many occupation values in the left DataFrame (users) \n",
    "# appear also in the right DataFrame (occupation_salary)\n",
    "print(\"Number of 'occupation' values shared between the two DataFrames:\\n{}\"\n",
    "      .format(users.occupation.isin(occupation_salary.occupation).value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Suppose we want to keep the records from the left DataFrame\n",
    "\"\"\"\n",
    "merged_left = pd.merge(users, occupation_salary, how='left')\n",
    "print(merged_left.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How many records do we expect the resulting merged DataFrame above to have?\n",
    "\"\"\"\n",
    "print(\"Actual number of records: {}\".format(merged_left.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Suppose we want to keep the records from the right DataFrame\n",
    "\"\"\"\n",
    "merged_right = pd.merge(users, occupation_salary, how='right')\n",
    "print(merged_right.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "How many records do we expect the resulting merged DataFrame above to have?\n",
    "\"\"\"\n",
    "print(\"Actual number of records: {}\".format(merged_right.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When we use 'merge' on column(s)-to-column(s) \n",
    "the indexes associated with the merging DataFrame objects are discarded.\n",
    "\"\"\"\n",
    "# Suppose we want to re-assign an Index object to the new merged DataFrame\n",
    "# using the previous non-default index of the left DataFrame (users)\n",
    "merged.index.name = 'user_id'\n",
    "\n",
    "merged.index = users.index[users.occupation.isin(occupation_salary.occupation)]\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose we want to create another DataFrame containing the (average) salary\n",
    "# for a set of 100 random users (i.e., not occupations)\n",
    "# 1. Let's pick 100 user_ids uniformly at random\n",
    "np.random.seed(23)\n",
    "\n",
    "random_users = np.random.choice(users.index, 100, replace=False)\n",
    "\n",
    "print(\"Selected number of random users = {}\"\n",
    "      .format(random_users.size))\n",
    "\n",
    "print(\"Q: Is there any duplicates? A: {}\"\n",
    "      .format(pd.Series(random_users).duplicated().any()))\n",
    "\n",
    "# print(\"Q: Is there any duplicates? A: {}\"\n",
    "#       .format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Let's do the same with the average salary.\n",
    "# This time we extract a 100-sample drawn from a Normal distribution.\n",
    "# To do so, we need to specify the two parameters of the distribution: \n",
    "# mean (mu) and standard deviation (sigma)\n",
    "mu = 35000\n",
    "sigma = 10000\n",
    "normal_salaries = np.random.normal(mu, sigma, 100)\n",
    "\n",
    "# Let's also round salaries to the 2nd decimal digit\n",
    "normal_salaries = np.round(normal_salaries, decimals=2)\n",
    "print(normal_salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Let's do the same with the average salary.\n",
    "# This time we extract a 100-sample drawn from a Normal distribution.\n",
    "# To do so, we need to specify the two parameters of the distribution: \n",
    "# mean (mu) and standard deviation (sigma)\n",
    "mu = 35000\n",
    "sigma = 10000\n",
    "normal_salaries = np.random.normal(loc=mu, scale=sigma, size=100)\n",
    "# Let's also round salaries to the 2nd decimal digit\n",
    "normal_salaries = np.round(normal_salaries, decimals=2)\n",
    "# Alternatively:\n",
    "# np.round(normal_salaries, decimals=2, out=normal_salaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Let's now create the dictionary containing two keys: 'user_id' and 'salary'\n",
    "# For each key, we associate the list of random_users and normal_salaries, respectively\n",
    "user_to_salary_data = {'user_id': random_users, 'salary': normal_salaries}\n",
    "\n",
    "# 4. We create the corresponding pandas DataFrame object\n",
    "user_to_salary = pd.DataFrame(user_to_salary_data)\n",
    "\n",
    "user_to_salary.set_index('user_id', inplace=True)\n",
    "\n",
    "print(\"User-Salary data:\\n{}\".format(user_to_salary.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Finally, merge (join) the original DataFrame (users) with this new one\n",
    "# Merging uses the index this time, rather than the column as before\n",
    "merged = pd.merge(users, user_to_salary, left_index=True, right_index=True)\n",
    "\n",
    "# Note that if you don't specify that you are merging on indexes, \n",
    "# merge raises an error!\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Finally, merge (join) the original DataFrame (users) with this new one\n",
    "# Merging uses the index this time, rather than the column as before\n",
    "#merged = pd.merge(users, user_to_salary, left_index=True, right_index=True)\n",
    "# Note that if you don't specify that you are merging on indexes, \n",
    "# merge raises an error!\n",
    "# This is equivalent to (join is 'left' by default):\n",
    "\n",
    "merged = users.join(user_to_salary, how='inner')\n",
    "\n",
    "print(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <code>pandas.merge</code> Arguments (1 of 2)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./img/pd_merge_args_1.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <code>pandas.merge</code> Arguments (1 of 2)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./img/pd_merge_args_2.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Options for the <code>how</code> Argument\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./img/pd_merge_how.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concatenating <code>DataFrame</code>s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can use the 'concat' function in pandas to append either rows or columns (if any) \n",
    "from one object to another. \n",
    "Let's grab two subsets of our DataFrame to see how this works.\n",
    "\"\"\"\n",
    "# Read in first 5 rows of users table\n",
    "users_first_5 = users.head()\n",
    "print(\"First 5 records of 'users':\\n{}\".format(users_first_5))\n",
    "print()\n",
    "# Read in the last 5 rows\n",
    "users_last_5 = users[-6:]\n",
    "print(\"Last 5 records of 'users':\\n{}\".format(users_last_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can use the 'concat' function in pandas to append either rows or columns (if any) \n",
    "from one object to another. \n",
    "Let's grab two subsets of our DataFrame to see how this works.\n",
    "\"\"\"\n",
    "# Read in first 5 rows of users table\n",
    "users_first_5 = users.head()\n",
    "print(\"First 5 records of 'users':\\n{}\".format(users_first_5))\n",
    "print()\n",
    "# Read in the last 5 rows\n",
    "users_last_5 = users[-5:]\n",
    "print(\"Last 5 records of 'users':\\n{}\".format(users_last_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stack \"_vertically_\" vs. \"_horizontally_\": <code>axis</code>\n",
    "\n",
    "-  By default, <code>**concat**</code> operates on <code>**axis=0**</code> (i.e., **rows**) and tells <code>**pandas**</code> to stack the second DataFrame under the first one (i.e., **vertically**). \n",
    "    -  In order for this to work, we need to make sure that both <code>**DataFrame**</code>s have the same column names and formats. \n",
    "\n",
    "-  Instead, <code>**axis=1**</code> will stack the **columns** in the second <code>**DataFrame**</code> to the right of the first one (i.e., **horizontally**). \n",
    "    - In this case, we want to make sure that the data we stack are related in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stack DataFrames vertically (i.e., using default axis=0)\n",
    "\"\"\"\n",
    "# Stack the two DataFrames 'users_first_5' and 'users_last_5' on top of each other\n",
    "vertical_stack = pd.concat([users_first_5, users_last_5], axis=0)\n",
    "print(vertical_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stack DataFrames horizontally (i.e., using axis=1)\n",
    "\"\"\"\n",
    "# Place the two DataFrames 'users_first_5' and 'users_last_5' side by side\n",
    "horizontal_stack = pd.concat([users_first_5, users_last_5], axis=1)\n",
    "print(horizontal_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since the row indexes for the two DataFrames 'users_first_5' and 'users_last_5' \n",
    "are not the same, 'concat' can't place them next to each other (NaN values occur). \n",
    "A workaround for this is to reindex our second DataFrame using the 'reset_index()' method.\n",
    "\"\"\"\n",
    "# Set the index of the second DataFrame to that of the first one\n",
    "users_last_5.set_index(users_first_5.index, inplace=True)\n",
    "horizontal_stack = pd.concat([users_first_5, users_last_5], axis=1)\n",
    "print(horizontal_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since the row indexes for the two DataFrames 'users_first_5' and 'users_last_5' \n",
    "are not the same, 'concat' can't place them next to each other (NaN values occur). \n",
    "A workaround for this is to reindex our second DataFrame using the 'reset_index()' method.\n",
    "\"\"\"\n",
    "# Set the index of the second DataFrame to that of the first one\n",
    "users_last_5.set_index(users_first_5.index, inplace=True)\n",
    "# Or:\n",
    "# users_last_5 = users_last_5.set_index(users_first_5.index)\n",
    "horizontal_stack = pd.concat([users_first_5, users_last_5], axis=1)\n",
    "print(horizontal_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_stack['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <code>pandas.concat</code> Arguments\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./img/pd_concat.png\">\n",
    "</p>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
